{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python 3.7\n",
    "\n",
    "This project exercises simple Machine Learning using the Iris dataset in using DecisionTreeClassifier, visualizing it, and then also creating two different classifiers of my own. \n",
    "\n",
    "The classifiers that I create are:\n",
    "* A Random Classifier, which simply selects a random label and uses that as the prediction.\n",
    "* A Nearest Neighbor Classifier, which uses the euclidean distance formula.\n",
    "\n",
    "addition from a previous notebook\n",
    "\n",
    "### Creating a Random Classifier\n",
    "\n",
    "This just guesses the label based off of random choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomGuessClassifier():\n",
    "    #we need a .fit(x_train, y_train) and .predict(x_test)\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        predictions = []      #a list of predictions is returned\n",
    "        for row in x_test:    #each row contains all the features in the training data.\n",
    "            label = random.choice(self.y_train)    #takes a random element in the y train list and selects it.\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Simple NearestNeighbor Classifier\n",
    "\n",
    "Pros: \n",
    "* pretty simple to understand\n",
    "\n",
    "Cons:\n",
    "* computationally intensive because it iterates through all testing points to find nearest neighbor\n",
    "* difficult to represent relationships between features, and to tell which features matter more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "# a is a point from training data, b is point from testing data\n",
    "def euc(a,b):\n",
    "    return distance.euclidean(a,b)\n",
    "    \n",
    "class My_KNN():\n",
    "    \n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        predictions = []     \n",
    "        for row in x_test:   \n",
    "            label = self.closest(row)    #feeds our \"closest\" function row, which is one row of features.\n",
    "            predictions.append(label)\n",
    "        return predictions\n",
    "    \n",
    "    def closest(self, row): \n",
    "        best_dist = euc(row, self.x_train[0]) #compares test point to first training point\n",
    "        best_index = 0\n",
    "        for i in range(1, len(self.x_train)): #compares the test point to all the other training points\n",
    "            dist = euc(row, self.x_train[i])\n",
    "            if(dist < best_dist):\n",
    "                best_dist = dist\n",
    "                best_index = i\n",
    "        return self.y_train[best_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "x = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to partition our data for a training set and a testing set. One to train our models and one to test them to see if they worked properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.5)  #use half the data for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell we are using a decision tree classifier to help classify what our labels are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "my_classifier = tree.DecisionTreeClassifier()    #you can use different classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#my_classifier = KNeighborsClassifier()          #this is a different classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using my own classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_classifier = RandomGuessClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_classifier = My_KNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will give the classifier the training data for the features and labels of the training data that we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_classifier.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the Y for all of the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 0 2 1 1 2 2 1 2 2 2 0 1 2 2 0 1 0 0 1 1 2 2 2 0 2 0 1 0 0 0 2\n",
      " 0 0 2 0 2 2 0 2 1 2 2 2 0 1 0 0 0 2 1 0 0 0 2 1 1 2 2 1 0 1 1 0 0 2 1 1 0\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "predictions = my_classifier.predict(x_test)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 0 2 1 2 2 2 1 2 2 2 0 1 2 1 0 2 0 0 1 1 2 2 2 0 2 0 1 0 0 0 2\n",
      " 0 0 2 0 2 2 0 2 1 2 2 2 0 1 0 0 0 2 1 0 0 0 2 2 1 2 2 1 0 1 1 0 0 2 1 1 0\n",
      " 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: [6.7 3.3 5.7 2.1]\n",
      "2: [6.3 2.5 5.  1.9]\n"
     ]
    }
   ],
   "source": [
    "#simple display of the data in order to check the thought tree decision process. \n",
    "#'sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "#for i in range(2):\n",
    "#    print(str(y_test[i]) + \": \" + str(x_test[i]))\n",
    "    \n",
    "print(str(y_test[10]) + \": \" + str(x_test[10]))\n",
    "print(str(y_test[41]) + \": \" + str(x_test[41]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How accurate is this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dtree_render.pdf'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = iris.feature_names\n",
    "\n",
    "import graphviz\n",
    "dot_data = tree.export_graphviz(my_classifier, out_file=None,  feature_names=features,  \n",
    "    class_names=iris.target_names,  filled=True, rounded=True, special_characters=True) \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render('dtree_render',view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
